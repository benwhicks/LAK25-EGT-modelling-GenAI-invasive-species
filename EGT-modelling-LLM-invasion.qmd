---
title: "short"
format: 
    html:
        code-fold: true
        code-summary: "Show the code"
editor: visual
---

Thinking - maybe make $b_s=0$ generally? It is percieved benefit. Maybe only in non GenAI? Maybe it's captured in cost / benefit ratio anyway??

Also, some kind of assortment would be good to model later. 

# EGT modelling of cooperation in classroom with LLM

## Modelling cooperation amongst peers

The model examines the relationship between peers in a learning environment providing feedback on each others work. In this model the agents are students which we will refer to as *learners*, and interactions are between learners choosing to provide and receive feedback from each other. We use $P$ to indicate the learner providing feedback, and $R$ the learner receiving feedback.

The strategy of interacting with other learners is outlined initially with a single parameter:

- $\Theta \in[0,1]$: Level of cooperative effort - that is how much effort a learner puts into providing feedback to another learner. The more effort towards cooperation the more benefit a peer receives from the feedback, and the more cost is incurred by the provider. $\Theta_P$ indicates the collaborative effort of the learner providing the feedback, and $\Theta_R$ the collaborative effort of the incoming feedback learner receiving the feedback. 

The environment is balanced by three parameters that guide the payoffs of the different strategies:

- $c$ - Cost of cooperation 
- $b_f$ - Benefit to yourself from others providing feedback / cooperating with you.
- $b_s$ - Benefit to yourself for providing others with feedback (i.e. you learn by providing feedback) 
- $a$ - Benefit to yourself if you are cooperating with someone with the same strategy as you (i.e. a social benefit, gravitation towards similar learners).

The model initially explores the scenario $c=3,b_f=4, b_s=2,a=1$, and will generally assume that $b_f > c$. We explore varying strategies and varying the value of $b_s$.

In this framework if a player with effort strategy $\Theta_R$ receives feedback from someone with effort strategy $\Theta_P$ they gain some *perceived* value, $V(R|P)$ (read as value to reciever $R$ when they meet provider $P$), out of sharing feedback with a peer: 

$$V(R|P) = \Theta_Pb_f  + \Theta_Rb_s  - \Theta_Rc +a[RP]$$

Where $a[RP]=a$ if $\Theta_R=\Theta_P$, and zero otherwise.

So the learner gains (or at least perceives to gain) knowledge from the feedback at a level dependent on the providers effort ($b_f\Theta_P$), as well as from providing feedback to them depending their own effort towards cooperation ($b_s\Theta_R$). They incur a cost relative to the effort they put into cooperation ($c\Theta_R$). 

Within this framework there are three strategies that agents can take that we will examine:

- $C$: Cooperation, where cooperative effort is highest, $\Theta_C=1$
- $T$: Token-effort, where cooperative effort is half-way, $\Theta_T=0.5$
- $F$: Free-rider, with no cooperative effort, $\Theta_F=0$

To introduce GenAI we provide two new strategy parameters, indicating the use of GenAI for providing feedback to others, or using GenAI to supplement feedback a learner receives: 

- $\Pi\in\{0,1\}$: The use of GenAI to provide feedback (1) or not (0). $\Pi_P$ indicates the choice of the provider of feedback. 
- $\Gamma \in \{0,1\}$: The use of GenAI to analyse your own work (1) or not (0). $\Gamma_R$ indicates the choice of the receiver. 

Additionally, there are two new environment parameters:

- $g_q$: The quality of the GenAI for feedback.  
- $g_c$: The reduced cost of providing feedback using AI.  

We use two values, $g_q=0.8$ to indicate slightly worse than what the peer would provide (with full effort), or $g_q=1.25$ for slightly better than what you would expect from peers. We use $g_c=0.1$ to indicate a low cost (compared to providing the feedback yourself).

This adds four new strategies of GenAI use:

- $N$: No GenAI use, $\Pi=0,\Gamma=0$.
- $S$: Using GenAI for only your own work, $\Pi=0,\Gamma=1$. This incurs an additional cost $c\times g_c$, but also provides a new benefit $b_f \times g_q$. However, this benefit cancels out any benefit of receiving feedback from someone who is using GenAI to produce the feedback. This means that when $\Gamma_R=1$ and $\Pi_P=1$ you can only get this benefit once. 
- $O$: Using GenAI only to provide feedback for others, $\Pi=1, \Gamma = 0$. This reduces the cost of providing feedback to $c \times g_c$ (as $0<g_c<1$), but also changes the value of the feedback to $b_f \times g_q$ instead of $b_f \times \Theta_P$.
- $B$: Using GenAI for yourself and others, $\Pi=1,\Gamma=1$. This combines the effects of $S$ and $O$.

We then combine the cooperation strategies and GenAI strategies. So $CN$ indicates that the learner is fully cooperating but not using GenAI for themselves or for providing feedback. $FS$ would indicate that a Free-rider (no effort towards cooperation) is using the GenAI for their own feedback. Note that the strategies $FO$ and $FB$ do not make sense - a Free-rider is not providing feedback for others so would not bother using GenAI for it. They might move towards $TS$ or $TB$ however. 

The new, extended value calculation is more complicated:

$$V(R|P) = (1-\Gamma_R)[\Pi_P g_q b_f + (1-\Pi_P)\Theta_P b_f] \\
+ \Gamma_R[g_q b_f-g_cc + (1-\Pi_P)\Theta_Pb_f] \\
+(1-\Pi_R)\Theta_R(b_s-c) \\
-\Pi_R g_c c + a_{RP}\\$$

Where $a[RP]$ returns the value of $a$ is $\Pi_R=\Pi_P$ and $\Theta_R=\Theta_P$, and zero otherwise. 

It takes a bit of looking, but the above formula, for $\Gamma_R=\Pi_R=\Pi_P=0$ this formula reduces to the non-GenAI scenario (note that $\Gamma_P$ is irrelevant for calculating the receivers payoff).
This game is instatiated in the code below:



``` {r creating-game}
library(tidyverse)
# library(EvolutionaryGames)

get_payoff <- function(
        # Strat_r: Receiving strat, Strat_p: Providing strat
    Strat_r, Strat_p, # expects rows of data frame with columns Eff, AIP, AIS
    p = list(
        # Parameters
        c = 2, # cost
        b_f = 3, # receive benefit - c <> b_r might depend on knowledge gap??
        b_s = 1, # self benefit
        g_q = 0.8, # LLM is 0.8 of decent feedback
        g_c = 0.1, # reduces cost of giving fb by this
        a = 0.5
    )) {
    # Relabelling to match model forumula
    Theta_R = Strat_r$Eff
    Theta_P = Strat_p$Eff
    Gamma_R = Strat_r$AIS
    Pi_R = Strat_r$AIP
    Pi_P = Strat_p$AIP
    
    # V.Gamma_R  <-  Gamma_R * (p$g_q * p$b_f - p$g_c * p$c)
    # V.Pi_P <-  (1-Pi_P) * max(Theta_P + Gamma_R * p$g_q, Gamma_R * p$g_q) * p$b_f + (Pi_P * p$g_c * p$b_f) * (1 - Gamma_R)
    # V.Pi_R <-  (1-Pi_R) * Theta_R * (p$b_s - p$c) - Pi_R * p$g_c * p$c

    V.Gamma_R0 <- (1-Gamma_R) * (
        Pi_P * p$g_q * p$b_f + (1-Pi_P) * (Theta_P * p$b_f)
        )
    V.Gamma_R1 <- Gamma_R*(p$g_q * p$b_f - p$g_c*p$c + (1-Pi_P) * Theta_P * p$b_f)
    V.Pi_R0 <- (1 - Pi_R) * Theta_R * (p$b_s - p$c)
    V.Pi_R1 <- -Pi_R * p$g_c * p$c
    
    same_Pi_strat <- if_else(Pi_R == Pi_P, 1, 0)
    same_Theta_strat <- if_else(Theta_R == Theta_P, 1, 0)
    same_strat = same_Pi_strat * same_Theta_strat * p$a
    
    V = V.Gamma_R0 + V.Gamma_R1 + V.Pi_R0 + V.Pi_R1 + same_strat
    return(V)
}
```

``` {r game-functions}
# game modelling
all_strategies <- tribble(
    ~Label, ~Eff, ~AIS, ~AIP,
    "CN", 1,   0, 0,
    "TN", 0.5, 0, 0,
    "FN", 0,   0, 0,
    "CS", 1,   1, 0,
    "TS", 0.5, 1, 0,
    "FS", 0,   1, 0,
    "CO", 1,   0, 1,
    "TO", 0.5, 0, 1,
    "FO", 0,   0, 1,
    "CB", 1,   1, 1,
    "TB", 0.5, 1, 1,
    "FB", 0,   1, 1
)
build_payoff_matrix <- function(
        S,
        # Parameters
        p = list(
            c = 2, # cost
            b_f = 2, # receive benefit
            b_s = 1, # self benefit
            g_q = 0.8, # LLM is 0.8 of decent feedback
            g_c = 0.1 # reduces cost of giving fb by this, but also reduces self benefit
        )
) {
    N_strategies <- nrow(S)
    payoffs <- array(rep(NA_real_, N_strategies*N_strategies), dim = c(N_strategies, N_strategies)) 
    for (R in 1:N_strategies) {
        for (P in 1:N_strategies) {
            payoffs[R,P] <- get_payoff(slice(S, R), slice(S, P), p)
        }
    }
    return(payoffs)
}

print_payoff_matrix <- function(M, stgys) {
    rownames(M) <- stgys
    colnames(M) <- stgys
    print(M)
}

fetch_payoff_matrix <- function(strategies, p, as_df = FALSE,
                                add_exp_V = FALSE) {
    # strategies: character vector, p: list
    S.df <- tibble(Label = strategies) |> # preserves order of strategies
        inner_join(all_strategies, by = "Label")
    payoff_matrix <- build_payoff_matrix(S.df, p)
    if (as_df) {
        colnames(payoff_matrix) <- strategies
        payoff_df <- as_tibble(payoff_matrix) |> 
            mutate(`Payoff to strategy:` = strategies) |> 
            select(`Payoff to strategy:`, everything())
        if (add_exp_V) {
            payoff_df <- payoff_df |> 
                rowwise() |> 
                mutate(ExpV = mean(c_across(where(is.numeric))))
        }
        return(payoff_df)
    } else {
        return(payoff_matrix)
    }
}
```



## No AI

Comparing Free-rider, Token (half effort) and Cooperator.

Initially with $c=4,b_f=5,b_s=2, a=1$. 

``` {r}
# Model params
        p = list(
            c = 3, # cost
            b_f = 4, # receive benefit
            b_s = 2, # self benefit
            g_q = 0.8, # LLM is 0.8 of decent feedback
            g_c = 0.1, # reduces cost of giving fb by this, but also reduces self benefit
            a = 1 # assortment
        )
glimpse(p)
```

Comparing just $FN$ and $CN$ (i.e. Hawk-Dove)
``` {r no-AI-2player-low-b_s}
strategies <- c("CN", "FN")
fetch_payoff_matrix(strategies, p, as_df = T)
```

The overall fitness of the population is given, for proportion of $p$ cooperators, by $W(p)=2p^2+2p(1-p)=2p$, so we want to have $p$ as high as possible. But from the point of view of the individual it is much nicer to choose $FN$. 

We can also include TN to see how the dynamics evolve over time. 

``` {r no-AI-3player-low-b_s}
strategies <- c("TN", "CN", "FN")
payoffs_df <- fetch_payoff_matrix(strategies, p, as_df = T)
payoffs_M <- fetch_payoff_matrix(strategies, p, as_df = F)
payoffs_df
```


``` {python}
import numpy as np
import matplotlib.pyplot as plt
from egttools.plotting.simplified import plot_replicator_dynamics_in_simplex, plot_pairwise_comparison_rule_dynamics_in_simplex_without_roots
from egttools.utils import calculate_stationary_distribution

payoffs = np.array(r.payoffs_M)
type_labels = r.strategies

fig, ax = plt.subplots(figsize=(15,10))
simplex, gradient_function, roots, roots_xy, stability = plot_replicator_dynamics_in_simplex(payoffs, nb_of_initial_points_for_root_search=100, 
ax=ax)

plot = (simplex.add_axis(ax=ax)
           .draw_triangle()
           .draw_gradients(zorder=0)
           .add_colorbar()
           .add_vertex_labels(type_labels)
           .draw_stationary_points(roots_xy, stability))

ax.axis('off')
ax.set_aspect('equal')
plt.xlim((-.05,1.05))
plt.ylim((-.02, simplex.top_corner + 0.05))
plt.show()
```

From this we can see that a population evenly mixed between the three strategies (in the centre of the triangle) would move away from the cooperative strategy $CN$ and slowly veer towards a population of free-riders, $FN$.

# Introducing AI

Three plots each for lower AI quality and higher AI quality. Many combos, so assumptions about behaviour:

``` {r}
fetch_payoff_matrix(
    all_strategies$Label, p, as_df = T, add_exp_V = T) |> arrange(ExpV)
```

- For any strategy $X$, $E[V(XS)]>E[V(XO)]>E[V(XN)]$, so GenAI strategies of none, $XN$, or others only, $XO$, are ignored. This is the high cost benefit ratio of using the GenAI for your own feedback. Maybe a supplementary proof??
- $FB=TB=CB$, so these will be treated as $FB$. Note that we have used $FB$ as this is not really putting in effort to feedback - the GenAI is being used to provide the feedback. 

``` {r}
interesting_strategies <- all_strategies |> 
    filter(!str_detect(Label, "O|N"), 
           Label != "TB", Label != "CB") |> 
    arrange(Label) |> 
    pull(Label)
interesting_strategies
```

This leaves `r length(interesting_strategies)`: `r str_c()`,

``` {r}
fetch_payoff_matrix(
    interesting_strategies, p, as_df = T, add_exp_V = T) |> arrange(ExpV)
```



### Increasing self benefit in no-AI environment

#### No AI: FN, CN, CS. $b_s=2$

Parameters:

``` {r}
p$b_s <- 2
p$g_q <- 1.25
glimpse(p)
```

``` {r}
strategies <- c("TN", "CN", "FN")
payoffs_M <- fetch_payoff_matrix(strategies, p, as_df = F)
```


``` {python}

import numpy as np
import matplotlib.pyplot as plt
from egttools.plotting.simplified import plot_replicator_dynamics_in_simplex, plot_pairwise_comparison_rule_dynamics_in_simplex_without_roots
from egttools.utils import calculate_stationary_distribution

payoffs = np.array(r.payoffs_M)
type_labels = r.strategies

fig, ax = plt.subplots(figsize=(15,10))
simplex, gradient_function, roots, roots_xy, stability = plot_replicator_dynamics_in_simplex(payoffs, nb_of_initial_points_for_root_search=100, 
ax=ax)

plot = (simplex.add_axis(ax=ax)
           .draw_triangle()
           .draw_gradients(zorder=0)
           # .add_colorbar()
           .add_vertex_labels(type_labels)
           .draw_stationary_points(roots_xy, stability))

ax.axis('off')
ax.set_aspect('equal')
plt.xlim((-.05,1.05))
plt.ylim((-.02, simplex.top_corner + 0.05))
plt.show()
```

#### No AI. $b_s=3$

Parameters:

``` {r}
p$g_q <- 1.25
p$b_s <- 3
glimpse(p)
```

``` {r}
strategies <- c("TN", "CN", "FN")
payoffs_M <- fetch_payoff_matrix(strategies, p, as_df = F)
```


``` {python}

import numpy as np
import matplotlib.pyplot as plt
from egttools.plotting.simplified import plot_replicator_dynamics_in_simplex, plot_pairwise_comparison_rule_dynamics_in_simplex_without_roots
from egttools.utils import calculate_stationary_distribution

payoffs = np.array(r.payoffs_M)
type_labels = r.strategies

fig, ax = plt.subplots(figsize=(15,10))
simplex, gradient_function, roots, roots_xy, stability = plot_replicator_dynamics_in_simplex(payoffs, nb_of_initial_points_for_root_search=100, 
ax=ax)

plot = (simplex.add_axis(ax=ax)
           .draw_triangle()
           .draw_gradients(zorder=0)
           # .add_colorbar()
           .add_vertex_labels(type_labels)
           .draw_stationary_points(roots_xy, stability))

ax.axis('off')
ax.set_aspect('equal')
plt.xlim((-.05,1.05))
plt.ylim((-.02, simplex.top_corner + 0.05))
plt.show()
```

#### No AI $b_s=4$

Parameters:

``` {r}
p$g_q <- 1.25
p$b_s <- 4
glimpse(p)
```

``` {r}
strategies <- c("TN", "CN", "FN")
payoffs_M <- fetch_payoff_matrix(strategies, p, as_df = F)
```


``` {python}

import numpy as np
import matplotlib.pyplot as plt
from egttools.plotting.simplified import plot_replicator_dynamics_in_simplex, plot_pairwise_comparison_rule_dynamics_in_simplex_without_roots
from egttools.utils import calculate_stationary_distribution

payoffs = np.array(r.payoffs_M)
type_labels = r.strategies

fig, ax = plt.subplots(figsize=(15,10))
simplex, gradient_function, roots, roots_xy, stability = plot_replicator_dynamics_in_simplex(payoffs, nb_of_initial_points_for_root_search=100, 
ax=ax)

plot = (simplex.add_axis(ax=ax)
           .draw_triangle()
           .draw_gradients(zorder=0)
           # .add_colorbar()
           .add_vertex_labels(type_labels)
           .draw_stationary_points(roots_xy, stability))

ax.axis('off')
ax.set_aspect('equal')
plt.xlim((-.05,1.05))
plt.ylim((-.02, simplex.top_corner + 0.05))
plt.show()
```


### Increasing self benefit in GenAI environment

#### Lower qual AI: FS, FB, CS. $b_s=2$

Parameters:

``` {r}
p$g_q <- 0.8
p$b_s <- 2
glimpse(p)
```

``` {r}
strategies <- c("FB", "CS", "FS")
payoffs_M <- fetch_payoff_matrix(strategies, p, as_df = F)
```


``` {python}

import numpy as np
import matplotlib.pyplot as plt
from egttools.plotting.simplified import plot_replicator_dynamics_in_simplex, plot_pairwise_comparison_rule_dynamics_in_simplex_without_roots
from egttools.utils import calculate_stationary_distribution

payoffs = np.array(r.payoffs_M)
type_labels = r.strategies

fig, ax = plt.subplots(figsize=(15,10))
simplex, gradient_function, roots, roots_xy, stability = plot_replicator_dynamics_in_simplex(payoffs, nb_of_initial_points_for_root_search=100, 
ax=ax)

plot = (simplex.add_axis(ax=ax)
           .draw_triangle()
           .draw_gradients(zorder=0)
           # .add_colorbar()
           .add_vertex_labels(type_labels)
           .draw_stationary_points(roots_xy, stability))

ax.axis('off')
ax.set_aspect('equal')
plt.xlim((-.05,1.05))
plt.ylim((-.02, simplex.top_corner + 0.05))
plt.show()
```

#### Lower qual AI: FS, FB, CS. $b_s=3$

Parameters:

``` {r}
p$g_q <- 0.8
p$b_s <- 3
glimpse(p)
```

``` {r}
strategies <- c("FB", "CS", "FS")
payoffs_M <- fetch_payoff_matrix(strategies, p, as_df = F)
```


``` {python}

import numpy as np
import matplotlib.pyplot as plt
from egttools.plotting.simplified import plot_replicator_dynamics_in_simplex, plot_pairwise_comparison_rule_dynamics_in_simplex_without_roots
from egttools.utils import calculate_stationary_distribution

payoffs = np.array(r.payoffs_M)
type_labels = r.strategies

fig, ax = plt.subplots(figsize=(15,10))
simplex, gradient_function, roots, roots_xy, stability = plot_replicator_dynamics_in_simplex(payoffs, nb_of_initial_points_for_root_search=100, 
ax=ax)

plot = (simplex.add_axis(ax=ax)
           .draw_triangle()
           .draw_gradients(zorder=0)
           # .add_colorbar()
           .add_vertex_labels(type_labels)
           .draw_stationary_points(roots_xy, stability))

ax.axis('off')
ax.set_aspect('equal')
plt.xlim((-.05,1.05))
plt.ylim((-.02, simplex.top_corner + 0.05))
plt.show()
```

#### Lower qual AI: FS, FB, CS. $b_s=4$

Parameters:

``` {r}
p$g_q <- 0.8
p$b_s <- 4
glimpse(p)
```

``` {r}
strategies <- c("FB", "CS", "FS")
payoffs_M <- fetch_payoff_matrix(strategies, p, as_df = F)
```


``` {python}

import numpy as np
import matplotlib.pyplot as plt
from egttools.plotting.simplified import plot_replicator_dynamics_in_simplex, plot_pairwise_comparison_rule_dynamics_in_simplex_without_roots
from egttools.utils import calculate_stationary_distribution

payoffs = np.array(r.payoffs_M)
type_labels = r.strategies

fig, ax = plt.subplots(figsize=(15,10))
simplex, gradient_function, roots, roots_xy, stability = plot_replicator_dynamics_in_simplex(payoffs, nb_of_initial_points_for_root_search=100, 
ax=ax)

plot = (simplex.add_axis(ax=ax)
           .draw_triangle()
           .draw_gradients(zorder=0)
           # .add_colorbar()
           .add_vertex_labels(type_labels)
           .draw_stationary_points(roots_xy, stability))

ax.axis('off')
ax.set_aspect('equal')
plt.xlim((-.05,1.05))
plt.ylim((-.02, simplex.top_corner + 0.05))
plt.show()
```




#### Higher qual AI: FS, FB, CS


# Invasion (Normal form 2x2game)

``` {python}
#| eval: false
# Invasion dynamics.
import numpy as np
import egttools as egt
import matplotlib.pyplot as plt

A = np.array(r.payoffs_M)
strategies = [egt.behaviors.NormalForm.TwoActions.Cooperator(), 
              egt.behaviors.NormalForm.TwoActions.Defector(), 
              egt.behaviors.NormalForm.TwoActions.TFT(),
              egt.behaviors.NormalForm.TwoActions.Pavlov(), 
              egt.behaviors.NormalForm.TwoActions.GRIM()]
strategy_labels = [strategy.type().replace("NFGStrategies::", '') for strategy in strategies]
game = egt.games.NormalFormGame(100, A, strategies)
Z= 100; beta=1
evolver = egt.analytical.PairwiseComparison(Z, game)
transition_matrix,fixation_probabilities = evolver.calculate_transition_and_fixation_matrix_sml(beta)
stationary_distribution = egt.utils.calculate_stationary_distribution(transition_matrix.transpose())
fig, ax = plt.subplots(figsize=(5, 5), dpi=150)
G = egt.plotting.draw_invasion_diagram(strategy_labels,
                                              1/Z, fixation_probabilities, stationary_distribution,
                                              node_size=600, 
                                              font_size_node_labels=8,
                                              font_size_edge_labels=8,
                                              font_size_sd_labels=8,
                                              edge_width=1,
                                              min_strategy_frequency=0.00001, 
                                              ax=ax)
plt.axis('off')
plt.show() # display

```